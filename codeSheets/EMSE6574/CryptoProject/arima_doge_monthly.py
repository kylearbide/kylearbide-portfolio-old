# -*- coding: utf-8 -*-
"""arima_DOGE_monthly.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CQVk5qBdYQcMfg6QvpV4fZjoUlerev5t
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from matplotlib.pyplot import figure
from datetime import datetime

from google.colab import drive
drive.mount('/content/drive')

DOGE_weekly = pd.read_csv('/content/drive/Shareddrives/Crypto SP500 /data for arima/DOGE-USD-weekly.csv')

DOGE_weekly['Date'] = pd.to_datetime(DOGE_weekly['Date'], infer_datetime_format=True)
DOGE_weekly

line = plt.plot(DOGE_weekly['Date'],DOGE_weekly['Close'], 'green',label = 'DOGE')
plt.title('DOGE-usd price')
plt.legend()
plt.xticks(rotation = -45)
plt.xlabel("Date")
plt.ylabel("DOGE price")
plt.show()

"""https://www.kaggle.com/freespirit08/time-series-for-beginners-with-arima

reference 
"""

# Commented out IPython magic to ensure Python compatibility.
from datetime import datetime
import numpy as np             #for numerical computations like log,exp,sqrt etc
import pandas as pd            #for reading & storing data, pre-processing
import matplotlib.pylab as plt #for visualization
#for making sure matplotlib plots are generated in Jupyter notebook itself
# %matplotlib inline             
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 10, 6

DOGE_weekly.drop(columns=(['Open','High','Low','Adj Close','Volume']),axis=1,inplace=True)
indexedDOGE_weekly=DOGE_weekly.set_index(['Date'])
indexedDOGE_weekly

#From the plot below, we can see that there is a Trend compoenent in th series. 
#Hence, we now check for stationarity of the data
#Determine rolling statistics
rolmean = indexedDOGE_weekly.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level
rolstd = indexedDOGE_weekly.rolling(window=12).std()
print(rolmean,rolstd)

#Plot rolling statistics
orig = plt.plot(indexedDOGE_weekly, color='blue', label='Original Price')
mean = plt.plot(rolmean, color='red', label='Rolling Mean')
std = plt.plot(rolstd, color='black', label='Rolling Std')
plt.legend(loc='best') #upper left location
plt.title('Rolling Mean & Standard Deviation')
plt.show(block=False)

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(indexedDOGE_weekly['Close'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

#P-value=0.515942 is extremely high. The time series is not stationary.

"""#Log Scale Transformation"""

#Estimating trend
indexedDOGE_weekly_logScale = np.log(indexedDOGE_weekly)
plt.plot(indexedDOGE_weekly_logScale)

#The below transformation is required to make series stationary
movingAverage = indexedDOGE_weekly_logScale.rolling(window=12).mean()
movingSTD = indexedDOGE_weekly_logScale.rolling(window=12).std()
plt.plot(indexedDOGE_weekly_logScale)
plt.plot(movingAverage, color='red')

datasetLogScaleMinusMovingAverage = indexedDOGE_weekly_logScale - movingAverage
datasetLogScaleMinusMovingAverage.head(12)

#Remove NAN values
datasetLogScaleMinusMovingAverage.dropna(inplace=True)
datasetLogScaleMinusMovingAverage.head(10)

def test_stationarity(timeseries):
    
    #Determine rolling statistics
    movingAverage = timeseries.rolling(window=12).mean()
    movingSTD = timeseries.rolling(window=12).std()
    
    #Plot rolling statistics
    orig = plt.plot(timeseries, color='blue', label='Original')
    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')
    std = plt.plot(movingSTD, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
    
    #Perform Dickey–Fuller test:
    print('Results of Dickey Fuller Test:')
    dftest = adfuller(timeseries['Close'], autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)

test_stationarity(datasetLogScaleMinusMovingAverage)

#Now P-value is much lower but still not enough to reject the hypothesis that our time series is not stationary,
#and the Critical values are closing to Test Statistic now.
#Looks good.

"""#Exponential Decay Transformation """

exponentialDecayWeightedAverage = indexedDOGE_weekly_logScale.ewm(halflife=3, min_periods=0, adjust=True).mean()
plt.plot(indexedDOGE_weekly_logScale)
plt.plot(exponentialDecayWeightedAverage, color='red')

datasetLogScaleMinusExponentialMovingAverage = indexedDOGE_weekly_logScale - exponentialDecayWeightedAverage
test_stationarity(datasetLogScaleMinusExponentialMovingAverage)

#p-value is not very good and the series for moving avg & std. dev. is not parallel to x-axis

"""#Time Shift Transformation """

datasetLogDiffShifting = indexedDOGE_weekly_logScale - indexedDOGE_weekly_logScale.shift()
plt.plot(datasetLogDiffShifting)

datasetLogDiffShifting.dropna(inplace=True)
test_stationarity(datasetLogDiffShifting)

#The rolling mean and rolling std are not parallelled to x-axis, which is not very good.

decomposition = seasonal_decompose(indexedDOGE_weekly_logScale, freq=3) 

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(indexedDOGE_weekly_logScale, label='Original')
plt.legend(loc='best')

plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')

plt.subplot(413)
plt.plot(seasonal, label='Seasonality')
plt.legend(loc='best')

plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')

plt.tight_layout()

#there can be cases where an observation simply consisted of trend & seasonality. In that case, there won't be 
#any residual component & that would be a null or NaN. Hence, we also remove such cases.

decomposedLogData = residual
decomposedLogData.dropna(inplace=True)
test_stationarity(decomposedLogData)

#The rolling mean and rolling std are parallelled to x-axis now, which is better than before.

decomposedLogData = residual
decomposedLogData.dropna(inplace=True)
test_stationarity(decomposedLogData)

"""#Plotting ACF & PACF"""

#ACF & PACF plots

lag_acf = acf(datasetLogDiffShifting, nlags=40)
lag_pacf = pacf(datasetLogDiffShifting, nlags=40, method='ols')

#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0, linestyle='--', color='red')
plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='red')
plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='red')
plt.title('Autocorrelation Function')            

#Plot PACF
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0, linestyle='--', color='red')
plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='red')
plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='red')
plt.title('Partial Autocorrelation Function')
            
plt.tight_layout()

"""

From the ACF graph, we see that curve touches y=0 and it crosses the first line at x=1 or 2 or 3 or 4. Thus, from theory, Q = 1 or 2 or 3 or 4. From the PACF graph, we see that curve touches y=0.0 line and crosses the first line at x=1 or 2 or 3 or 4. Thus, from theory, P = 1 or 2 or 3 or 4.

ARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models wi:ll give a value of RSS. Lower RSS values indicate a better model.
"""

#AR Model
#making order=() gives RSS=
model = ARIMA(indexedDOGE_weekly_logScale, order=(2,1,0))
results_AR = model.fit(disp=-1)
plt.plot(datasetLogDiffShifting)
plt.plot(results_AR.fittedvalues, color='red')
plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['Close'])**2))
print('Plotting AR model')

#MA Model
model = ARIMA(indexedDOGE_weekly_logScale, order=(0,1,2))
results_MA = model.fit(disp=-1)
plt.plot(datasetLogDiffShifting)
plt.plot(results_MA.fittedvalues, color='red')
plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['Close'])**2))
print('Plotting MA model')

# AR+I+MA = ARIMA model
model = ARIMA(indexedDOGE_weekly_logScale, order=(2,1,2))
results_ARIMA = model.fit(disp=-1)
plt.plot(datasetLogDiffShifting)
plt.plot(results_ARIMA.fittedvalues, color='red')
plt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - datasetLogDiffShifting['Close'])**2))
print('Plotting ARIMA model')

#RSS of ARIMA = 15.38 < rss of ar and ma, ARIMA model performs better on our time series.
#But the rss is still very large.

"""#Prediction & Reverse transformations"""

predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)
print(predictions_ARIMA_diff.head())

#Convert to cumulative sum
predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
print(predictions_ARIMA_diff_cumsum)

predictions_ARIMA_log = pd.Series(indexedDOGE_weekly_logScale['Close'].iloc[0], index=indexedDOGE_weekly_logScale.index)
predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)
predictions_ARIMA_log.head()

# Inverse of log is exp.
predictions_ARIMA = np.exp(predictions_ARIMA_log)
plt.plot(indexedDOGE_weekly)
plt.plot(predictions_ARIMA)

indexedDOGE_weekly_logScale

#We have 378(existing data of 7 yrs in weeks) data points. 
#And we want to forecast for additional 150 data points for 3 yrs.
results_ARIMA.plot_predict(1,528)